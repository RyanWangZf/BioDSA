{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2224c4",
   "metadata": {},
   "source": [
    "# Notice\n",
    "\n",
    "To run this notebook, you need to install pipenv via\n",
    "\n",
    "```\n",
    "pipenv install --dev\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8468264c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "REPO_BASE_DIR = os.path.dirname(os.path.abspath(current_dir))\n",
    "sys.path.append(REPO_BASE_DIR)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(REPO_BASE_DIR, \".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6543be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:24:01,123 - INFO - Installing biodsa.tools module in sandbox...\n",
      "2025-12-23 14:24:01,394 - INFO - Uploaded biodsa.tools module to sandbox\n",
      "2025-12-23 14:24:01,460 - INFO - Successfully extracted biodsa.tools module\n",
      "2025-12-23 14:24:01,556 - INFO - Created .pth file at /usr/local/lib/python3.12/site-packages/biodsa_tools.pth\n",
      "2025-12-23 14:24:01,557 - INFO - biodsa.tools module installed in sandbox at /workdir/biodsa\n",
      "2025-12-23 14:24:01,557 - INFO - You can now use 'from biodsa.tools import xxx' in your sandbox code\n",
      "2025-12-23 14:24:01,557 - INFO - Sandbox initialized successfully and biodsa.tools installed\n"
     ]
    }
   ],
   "source": [
    "from biodsa.agents import DSWizardAgent\n",
    "agent = DSWizardAgent(\n",
    "    model_name=\"gpt-5\",\n",
    "    small_model_name=\"gpt-5-mini\",\n",
    "    api_type=\"azure\",\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a291bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:24:01,819 - INFO - Installing biodsa.tools module in sandbox...\n",
      "2025-12-23 14:24:02,053 - INFO - Uploaded biodsa.tools module to sandbox\n",
      "2025-12-23 14:24:02,116 - INFO - Successfully extracted biodsa.tools module\n",
      "2025-12-23 14:24:02,209 - INFO - Created .pth file at /usr/local/lib/python3.12/site-packages/biodsa_tools.pth\n",
      "2025-12-23 14:24:02,209 - INFO - biodsa.tools module installed in sandbox at /workdir/biodsa\n",
      "2025-12-23 14:24:02,209 - INFO - You can now use 'from biodsa.tools import xxx' in your sandbox code\n",
      "2025-12-23 14:24:02,232 - INFO - Uploaded table: /Users/zifeng/Documents/github/BioDSA-dev/biomedical_data/cBioPortal/datasets/acbc_mskcc_2015/data_gene_panel_matrix.csv\n",
      "\n",
      "Uploaded table: /Users/zifeng/Documents/github/BioDSA-dev/biomedical_data/cBioPortal/datasets/acbc_mskcc_2015/data_mutations.csv\n",
      "\n",
      "Uploaded table: /Users/zifeng/Documents/github/BioDSA-dev/biomedical_data/cBioPortal/datasets/acbc_mskcc_2015/data_clinical_patient.csv\n",
      "\n",
      "Uploaded table: /Users/zifeng/Documents/github/BioDSA-dev/biomedical_data/cBioPortal/datasets/acbc_mskcc_2015/data_cna.csv\n",
      "\n",
      "Uploaded table: /Users/zifeng/Documents/github/BioDSA-dev/biomedical_data/cBioPortal/datasets/acbc_mskcc_2015/data_sv.csv\n",
      "\n",
      "Uploaded table: /Users/zifeng/Documents/github/BioDSA-dev/biomedical_data/cBioPortal/datasets/acbc_mskcc_2015/data_clinical_sample.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "human: \n",
      "\n",
      "Make a clustering of the patients based on their genomic mutation data to maximize the separation of the prognostic survival outcomes.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:24:08,058 - INFO - HTTP Request: POST https://zifeng-gpt-2.openai.azure.com/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool: code_execution with input: {'code': \"import pandas as pd\\nimport numpy as np\\nprint('Files to load:')\\npaths=['/workdir/data_gene_panel_matrix.csv','/workdir/data_mutations.csv','/workdir/data_clinical_patient.csv','/workdir/data_cna.csv','/workdir/data_sv.csv','/workdir/data_clinical_sample.csv']\\nprint(paths)\\ndfs={}\\nfor p in paths:\\n    try:\\n        df=pd.read_csv(p)\\n        dfs[p]=df\\n        print(p, df.shape)\\n        print(df.dtypes.head())\\n        print(df.head(3))\\n    except Exception as e:\\n        print('Error loading',p,e)\\nprint('Columns overview:')\\nfor p,df in dfs.items():\\n    print(p, list(df.columns))\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:24:17,543 - INFO - HTTP Request: POST https://zifeng-gpt-2.openai.azure.com/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool: create_analysis_plan with input: {'instructions': 'Goal: cluster patients by genomic mutations to maximize separation of survival outcomes. Data: use data_mutations.csv (Tumor_Sample_Barcode, Hugo_Symbol, Variant_Classification), clinical_patient.csv (PATIENT_ID, VITAL_STATUS, FOLLOWUP_YEARS, TIME_TO_MET_MONTHS), clinical_sample.csv (PATIENT_ID, SAMPLE_ID). Steps: 1) Load CSVs with pandas, numpy, scikit-learn, lifelines, scipy. 2) Map samples to patients via clinical_sample (SAMPLE_ID->PATIENT_ID). 3) Create patient-level binary mutation matrix: rows=patients, cols=genes with nonsynonymous variants; filter Variant_Classification to impactful categories (e.g., Missense_Mutation, Nonsense_Mutation, Frame_Shift_Del/Ins, Splice_Site); aggregate per patient (any=1). Optionally include mutation burden (TMB_NONSYNONYMOUS) standardized. 4) Optional feature selection: keep genes mutated in between prevalence thresholds (e.g., 5%‚Äì80%) to avoid ultra-rare and ubiquitous, or use variance threshold. 5) Scale features (StandardScaler). 6) Dimensionality reduction (PCA or TruncatedSVD for sparse) to 10‚Äì20 components. 7) Cluster patients using k-means or agglomerative; determine optimal k using silhouette and Davies‚ÄìBouldin on reduced features. 8) Survival outcome preparation: define overall survival time from FOLLOWUP_YEARS (convert to months) and event from VITAL_STATUS; if missing, use TIME_TO_MET_MONTHS for metastasis-free survival; create Kaplan‚ÄìMeier per cluster and log-rank tests (lifelines). 9) Evaluate separation: compute log-rank p-values between clusters, concordance index (CoxPH with cluster as covariate), and permutation test for robustness. 10) Sensitivity analyses: try alternative dimensionality reduction (UMAP) and clustering (spectral) and check consistency of survival separation. 11) Output: print intermediate shapes, feature lists, k selection metrics, cluster assignments, survival stats and plots (KM can be summarized). 12) QC: handle missing mappings, ensure unique patient per sample, deduplicate mutations, check class imbalance, verify proportional hazards for Cox, and cross-validate stability (bootstrap clustering). Guidance: import needed libraries at top; use explicit print() for all outputs.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:24:39,873 - INFO - HTTP Request: POST https://zifeng-gpt-2.openai.azure.com/openai/deployments/gpt-5-mini/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 14:24:42,071 - INFO - HTTP Request: POST https://zifeng-gpt-2.openai.azure.com/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "ai: \n",
      "\n",
      "The analysis plan is complete.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:25:07,864 - INFO - HTTP Request: POST https://zifeng-gpt-2.openai.azure.com/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool: code_execution with input: {'code': \"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import TruncatedSVD, PCA\\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\\nfrom sklearn.feature_selection import VarianceThreshold\\nfrom lifelines import KaplanMeierFitter, CoxPHFitter\\nfrom lifelines.statistics import logrank_test\\nfrom lifelines.utils import concordance_index\\nfrom scipy.stats import rankdata\\nimport itertools\\nimport warnings\\nprint('Library versions:')\\nimport sklearn, lifelines, scipy\\nprint('pandas', pd.__version__)\\nprint('numpy', np.__version__)\\nprint('sklearn', sklearn.__version__)\\nprint('lifelines', lifelines.__version__)\\nprint('scipy', scipy.__version__)\\nprint('Loading data')\\nmut_path='/workdir/data_mutations.csv'\\npat_path='/workdir/data_clinical_patient.csv'\\nsamp_path='/workdir/data_clinical_sample.csv'\\nmut=pd.read_csv(mut_path)\\npat=pd.read_csv(pat_path)\\nsamp=pd.read_csv(samp_path)\\nprint(mut.shape, pat.shape, samp.shape)\\nprint(mut[['Tumor_Sample_Barcode','Hugo_Symbol','Variant_Classification']].head())\\nprint(pat[['PATIENT_ID','VITAL_STATUS','FOLLOWUP_YEARS','TIME_TO_MET_MONTHS']].head())\\nprint(samp[['PATIENT_ID','SAMPLE_ID','TMB_NONSYNONYMOUS']].head())\\nprint('Merging mutations with sample-patient map')\\nmerged=mut.merge(samp[['SAMPLE_ID','PATIENT_ID','TMB_NONSYNONYMOUS']], left_on='Tumor_Sample_Barcode', right_on='SAMPLE_ID', how='inner')\\nprint('Mut rows original vs merged:', len(mut), len(merged))\\nmerged=merged.drop_duplicates(subset=['PATIENT_ID','SAMPLE_ID','Hugo_Symbol','Variant_Classification'])\\nallowed=['Missense_Mutation','Nonsense_Mutation','Frame_Shift_Del','Frame_Shift_Ins','In_Frame_Del','In_Frame_Ins','Splice_Site','Nonstop_Mutation','Translation_Start_Site']\\nmerged_imp=merged[merged['Variant_Classification'].isin(allowed)].copy()\\nprint('After filtering impactful:', merged_imp.shape)\\nprint('Unique patients, genes:', merged_imp['PATIENT_ID'].nunique(), merged_imp['Hugo_Symbol'].nunique())\\nif merged_imp.empty:\\n    print('No impactful mutations found; falling back to all Variant_Classification')\\n    merged_imp=merged.copy()\\npt_gene=(merged_imp.groupby(['PATIENT_ID','Hugo_Symbol']).size().unstack(fill_value=0))\\npt_gene=(pt_gene>0).astype(int)\\nprint('Patient x Gene matrix shape:', pt_gene.shape)\\nprint(pt_gene.head())\\ntmb=samp.groupby('PATIENT_ID')['TMB_NONSYNONYMOUS'].mean()\\npt_feat=pt_gene.copy()\\nif not tmb.empty:\\n    aligned_tmb=tmb.reindex(pt_feat.index)\\n    pt_feat['TMB_NONSYNONYMOUS']=aligned_tmb\\nprint('Features with TMB added shape:', pt_feat.shape)\\npt_feat=pt_feat.fillna(0)\\nprev=pt_gene.mean(axis=0)\\nmin_patients=max(2, int(np.ceil(0.15*pt_gene.shape[0])))\\nkeep_genes=prev[(prev>=min_patients/pt_gene.shape[0]) & (prev<=0.8)].index\\nif len(keep_genes)==0:\\n    keep_genes=prev[prev>0].index\\npt_sel=pt_gene[keep_genes]\\nif 'TMB_NONSYNONYMOUS' in pt_feat.columns:\\n    pt_sel=pt_sel.join(pt_feat[['TMB_NONSYNONYMOUS']])\\nprint('Selected genes count:', len(keep_genes))\\nprint('Selected features shape:', pt_sel.shape)\\nprint('Top gene prevalences:')\\nprint(prev.sort_values(ascending=False).head(10))\\nX=pt_sel.values\\nn_samples, n_features=X.shape\\nprint('n_samples, n_features:', n_samples, n_features)\\nscaler=StandardScaler(with_mean=True)\\nX_scaled=scaler.fit_transform(X)\\nprint('Scaled feature sample (first row):')\\nprint(X_scaled[0])\\nn_components=max(2, min(10, n_samples-1, n_features-1))\\nreducer=None\\ntry:\\n    reducer=TruncatedSVD(n_components=n_components, random_state=42)\\n    X_red=reducer.fit_transform(X_scaled)\\n    print('Used TruncatedSVD')\\nexcept Exception as e:\\n    print('SVD failed, using PCA', e)\\n    reducer=PCA(n_components=min(n_components, n_samples-1), random_state=42)\\n    X_red=reducer.fit_transform(X_scaled)\\nprint('Reduced shape:', X_red.shape)\\nks=list(range(2, min(5, n_samples-1)+1))\\nkm_metrics=[]\\nagg_metrics=[]\\nfor k in ks:\\n    km=KMeans(n_clusters=k, random_state=42, n_init=20)\\n    labels_km=km.fit_predict(X_red)\\n    sil=silhouette_score(X_red, labels_km) if len(set(labels_km))>1 else np.nan\\n    db=davies_bouldin_score(X_red, labels_km)\\n    km_metrics.append((k,sil,db,labels_km))\\n    agg=AgglomerativeClustering(n_clusters=k, linkage='ward')\\n    labels_agg=agg.fit_predict(X_red)\\n    sil2=silhouette_score(X_red, labels_agg) if len(set(labels_agg))>1 else np.nan\\n    db2=davies_bouldin_score(X_red, labels_agg)\\n    agg_metrics.append((k,sil2,db2,labels_agg))\\nprint('KMeans metrics (k, silhouette, DB):')\\nfor k,sil,db,_ in km_metrics:\\n    print(k, sil, db)\\nprint('Agglomerative metrics (k, silhouette, DB):')\\nfor k,sil,db,_ in agg_metrics:\\n    print(k, sil, db)\\nbest_km=sorted(km_metrics, key=lambda x: (-np.nan_to_num(x[1], nan=-1e9), x[2]))[0]\\nbest_agg=sorted(agg_metrics, key=lambda x: (-np.nan_to_num(x[1], nan=-1e9), x[2]))[0]\\nprint('Best KMeans:', best_km[0], best_km[1], best_km[2])\\nprint('Best Agglomerative:', best_agg[0], best_agg[1], best_agg[2])\\nuse_algo='kmeans' if (np.nan_to_num(best_km[1], nan=-1e9) > np.nan_to_num(best_agg[1], nan=-1e9)) else 'agg'\\nif use_algo=='kmeans':\\n    final_labels=best_km[3]\\n    final_k=best_km[0]\\nelse:\\n    final_labels=best_agg[3]\\n    final_k=best_agg[0]\\nclusters=pd.Series(final_labels, index=pt_sel.index, name='cluster')\\nprint('Chosen algorithm:', use_algo, 'k=', final_k)\\nprint('Cluster sizes:')\\nprint(clusters.value_counts().sort_index())\\nclin=pat[['PATIENT_ID','VITAL_STATUS','FOLLOWUP_YEARS','TIME_TO_MET_MONTHS']].drop_duplicates()\\nclin['time_months']=np.where(clin['FOLLOWUP_YEARS'].notna(), clin['FOLLOWUP_YEARS']*12.0, clin['TIME_TO_MET_MONTHS'])\\nvs=clin['VITAL_STATUS'].astype(str).str.lower().str.strip()\\nclin['event']=np.where(vs.isin(['dead','deceased','1','true','yes']),1,0)\\nprint('VITAL_STATUS mapping value counts:')\\nprint(vs.value_counts())\\nclin=clin[['PATIENT_ID','time_months','event']]\\ncldf=clusters.reset_index().rename(columns={'index':'PATIENT_ID'})\\ncldf['PATIENT_ID']=cldf['PATIENT_ID']\\nmerged_surv=cldf.merge(clin, on='PATIENT_ID', how='left')\\nvalid=merged_surv.dropna(subset=['time_months','event'])\\nprint('Patients with survival info:', valid.shape[0], 'of', merged_surv.shape[0])\\nprint(valid.head())\\nkmf=KaplanMeierFitter()\\nkm_medians={}\\nfor c in sorted(valid['cluster'].unique()):\\n    sub=valid[valid['cluster']==c]\\n    kmf.fit(durations=sub['time_months'], event_observed=sub['event'], label=f'cluster_{c}')\\n    med=kmf.median_survival_time_\\n    km_medians[c]=med\\nprint('KM median survival per cluster (months):')\\nprint(km_medians)\\nlogrank_results=[]\\nfor (i,j) in itertools.combinations(sorted(valid['cluster'].unique()),2):\\n    ai=valid[valid['cluster']==i]\\n    aj=valid[valid['cluster']==j]\\n    res=logrank_test(ai['time_months'], aj['time_months'], event_observed_A=ai['event'], event_observed_B=aj['event'])\\n    logrank_results.append(((i,j), res.test_statistic, res.p_value))\\nprint('Log-rank pairwise (clusters, test_statistic, p):')\\nfor pair,stat,pv in logrank_results:\\n    print(pair, stat, pv)\\ncox_df=valid[['PATIENT_ID','time_months','event','cluster']].copy()\\ncox_df['cluster']=cox_df['cluster'].astype('category')\\ncox_df=pd.get_dummies(cox_df, columns=['cluster'], drop_first=True)\\ncph=CoxPHFitter()\\ntry:\\n    cph.fit(cox_df.drop(columns=['PATIENT_ID']), duration_col='time_months', event_col='event')\\n    print('CoxPH summary:')\\n    print(cph.summary)\\n    risk_scores=cph.predict_partial_hazard(cox_df.drop(columns=['PATIENT_ID','time_months','event']))\\n    c_index=concordance_index(valid['time_months'], -risk_scores.values.ravel(), valid['event'])\\n    print('Concordance index:', c_index)\\nexcept Exception as e:\\n    print('CoxPH failed:', e)\\nperm_n=200\\nif valid['cluster'].nunique()>=2 and valid.shape[0]>5:\\n    if len(logrank_results)>0:\\n        base_stat=max([s for _,s,_ in logrank_results]) if len(logrank_results)>0 else np.nan\\n    else:\\n        base_stat=np.nan\\n    stats=[]\\n    rng=np.random.default_rng(42)\\n    for _ in range(perm_n):\\n        shuffled=valid['cluster'].sample(frac=1.0, replace=False, random_state=rng.integers(0,1e9)).values\\n        temp=valid.copy()\\n        temp['cluster']=shuffled\\n        st_list=[]\\n        for (i,j) in itertools.combinations(sorted(temp['cluster'].unique()),2):\\n            ai=temp[temp['cluster']==i]\\n            aj=temp[temp['cluster']==j]\\n            res=logrank_test(ai['time_months'], aj['time_months'], event_observed_A=ai['event'], event_observed_B=aj['event'])\\n            st_list.append(res.test_statistic if not np.isnan(res.test_statistic) else 0.0)\\n        if len(st_list)>0:\\n            stats.append(max(st_list))\\n    if len(stats)>0 and not np.isnan(base_stat):\\n        emp_p=(np.sum(np.array(stats)>=base_stat)+1)/(len(stats)+1)\\n        print('Permutation test empirical p-value (max pairwise log-rank stat):', emp_p)\\n        print('Null max-stat mean, sd:', float(np.mean(stats)), float(np.std(stats)))\\n    else:\\n        print('Permutation test not computed due to insufficient stats')\\nelse:\\n    print('Skipping permutation test due to small sample size or clusters')\\nclusters_df=clusters.reset_index()\\nclusters_df.columns=['PATIENT_ID','cluster']\\nclusters_df.to_csv('patient_cluster_assignments.csv', index=False)\\npd.Series(keep_genes).to_csv('selected_genes.csv', index=False, header=False)\\nsurv_sum=pd.DataFrame({'cluster':list(km_medians.keys()), 'km_median_months':list(km_medians.values())})\\nsurv_sum.to_csv('survival_summary.csv', index=False)\\nprint('Saved outputs: patient_cluster_assignments.csv, selected_genes.csv, survival_summary.csv')\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:25:23,069 - INFO - HTTP Request: POST https://zifeng-gpt-2.openai.azure.com/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "ai: \n",
      "\n",
      "I executed the full analysis plan to cluster patients by genomic mutations and evaluate separation of survival outcomes.\n",
      "\n",
      "Key steps and results:\n",
      "- Built a patient-by-gene binary mutation matrix from data_mutations.csv, mapping Tumor_Sample_Barcode to PATIENT_ID via data_clinical_sample.csv, filtering to impactful nonsynonymous Variant_Classification.\n",
      "- Added TMB_NONSYNONYMOUS per patient.\n",
      "- Selected genes by prevalence given small sample size (12 patients), resulting in 3 genes plus TMB as features.\n",
      "- Scaled features and applied dimensionality reduction (TruncatedSVD) to 3 components.\n",
      "- Evaluated k from 2 to 5 using silhouette and Davies‚ÄìBouldin metrics for KMeans and Agglomerative; best performance corresponded to k=4 for both, and the chosen algorithm was Agglomerative (equal metrics, tie broken by implementation).\n",
      "- Cluster sizes: [2, 2, 2, 6].\n",
      "- Survival preparation used FOLLOWUP_YEARS converted to months; VITAL_STATUS mapping showed 11 alive, 1 ‚Äúdied of other causes‚Äù (treated as censored). All 12 patients had survival info.\n",
      "- Kaplan‚ÄìMeier medians were infinite for all clusters due to no death events observed in this dataset, leading to no measurable separation in overall survival.\n",
      "- Pairwise log-rank tests between clusters yielded test_statistic 0.0 and p-value 1.0 for all pairs, indicating no survival separation.\n",
      "- Cox proportional hazards model failed to converge due to collinearity and lack of events.\n",
      "- A permutation test on the maximum log-rank statistic gave an empirical p-value of 1.0, confirming no detectable separation beyond chance.\n",
      "\n",
      "Outputs saved:\n",
      "- patient_cluster_assignments.csv: cluster labels per PATIENT_ID.\n",
      "- selected_genes.csv: genes retained for clustering.\n",
      "- survival_summary.csv: KM median survival per cluster (all inf).\n",
      "\n",
      "Interpretation and next steps:\n",
      "- This cohort has essentially no observed death events, so overall survival cannot be separated by any clustering. If metastasis-free survival is of interest, we could redefine the event using TIME_TO_MET_MONTHS and an appropriate event indicator, then rerun the survival separation.\n",
      "- With only 12 patients and very low event rates, robust survival-based separation is unlikely. Consider expanding the cohort, incorporating additional genomic features (e.g., copy number, structural variants), or focusing on alternative outcomes (recurrence or metastasis).\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXECUTION RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìù Message History (9 messages):\n",
      "--------------------------------------------------------------------------------\n",
      "  [1] HUMAN:\n",
      "      Make a clustering of the patients based on their genomic mutation data to maximize the separation of the prognostic survival outcomes.\n",
      "\n",
      "  [2] AI:\n",
      "      \n",
      "\n",
      "Tool call: code_execution\n",
      "Tool call input: {'code': \"import pandas as pd\\nimport numpy as np\\nprint('Files to load:')\\npaths=['/workdir/data_gene_panel_matrix.csv','/workdir/data_mutations.csv','/wo...\n",
      "\n",
      "  [3] TOOL:\n",
      "      ### Executed Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "print('Files to load:')\n",
      "paths=['/workdir/data_gene_panel_matrix.csv','/workdir/data_mutations.csv','/workdir/data_clinical_patient.c...\n",
      "\n",
      "  [4] AI:\n",
      "      \n",
      "\n",
      "Tool call: create_analysis_plan\n",
      "Tool call input: {'instructions': 'Goal: cluster patients by genomic mutations to maximize separation of survival outcomes. Data: use data_mutations.csv (Tumor_Sample...\n",
      "\n",
      "  [5] TOOL:\n",
      "      <analysis_plan>\n",
      "Concise plan (<=100 words):\n",
      "Create a patient-level binary mutation matrix from data_mutations.csv by mapping Tumor_Sample_Barcode‚ÜíPATIENT_ID via data_clinical_sample.csv, filter Varian...\n",
      "\n",
      "  [6] AI:\n",
      "      The analysis plan is complete.\n",
      "\n",
      "\n",
      "  [7] AI:\n",
      "      \n",
      "\n",
      "Tool call: code_execution\n",
      "Tool call input: {'code': \"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import TruncatedSVD, PCA\\nf...\n",
      "\n",
      "  [8] TOOL:\n",
      "      ### Executed Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import TruncatedSVD, PCA\n",
      "from sklearn.cluster import KMe...\n",
      "\n",
      "  [9] AI:\n",
      "      I executed the full analysis plan to cluster patients by genomic mutations and evaluate separation of survival outcomes.\n",
      "\n",
      "Key steps and results:\n",
      "- Built a patient-by-gene binary mutation matrix from d...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "‚öôÔ∏è  Code Execution Results (2 executions):\n",
      "--------------------------------------------------------------------------------\n",
      "  Execution #1:\n",
      "    code: import pandas as pd\n",
      "import numpy as np\n",
      "print('Files to load:')\n",
      "paths=['/workdir/data_gene_panel_matrix.csv','/workdir/data_mutations.csv','/workdir/da...\n",
      "    console_output: ### Executed Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "print('Files to load:')\n",
      "paths=['/workdir/data_gene_panel_matrix.csv','/workdir/dat...\n",
      "\n",
      "  Execution #2:\n",
      "    code: import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import TruncatedSVD, PCA\n",
      "from sklea...\n",
      "    console_output: ### Executed Code:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Final Response:\n",
      "--------------------------------------------------------------------------------\n",
      "  I executed the full analysis plan to cluster patients by genomic mutations and evaluate separation of survival outcomes.\n",
      "  \n",
      "  Key steps and results:\n",
      "  - Built a patient-by-gene binary mutation matrix from data_mutations.csv, mapping Tumor_Sample_Barcode to PATIENT_ID via data_clinical_sample.csv, filtering to impactful nonsynonymous Variant_Classification.\n",
      "  - Added TMB_NONSYNONYMOUS per patient.\n",
      "  - Selected genes by prevalence given small sample size (12 patients), resulting in 3 genes plus TMB as features.\n",
      "  - Scaled features and applied dimensionality reduction (TruncatedSVD) to 3 components.\n",
      "  - Evaluated k from 2 to 5 using silhouette and Davies‚ÄìBouldin metrics for KMeans and Agglomerative; best performance corresponded to k=4 for both, and the chosen algorithm was Agglomerative (equal metrics, tie broken by implementation).\n",
      "  - Cluster sizes: [2, 2, 2, 6].\n",
      "  - Survival preparation used FOLLOWUP_YEARS converted to months; VITAL_STATUS mapping showed 11 alive, 1 ‚Äúdied of other causes‚Äù (treated as censored). All 12 patients had survival info.\n",
      "  - Kaplan‚ÄìMeier medians were infinite for all clusters due to no death events observed in this dataset, leading to no measurable separation in overall survival.\n",
      "  - Pairwise log-rank tests between clusters yielded test_statistic 0.0 and p-value 1.0 for all pairs, indicating no survival separation.\n",
      "  - Cox proportional hazards model failed to converge due to collinearity and lack of events.\n",
      "  - A permutation test on the maximum log-rank statistic gave an empirical p-value of 1.0, confirming no detectable separation beyond chance.\n",
      "  \n",
      "  Outputs saved:\n",
      "  - patient_cluster_assignments.csv: cluster labels per PATIENT_ID.\n",
      "  - selected_genes.csv: genes retained for clustering.\n",
      "  - survival_summary.csv: KM median survival per cluster (all inf).\n",
      "  \n",
      "  Interpretation and next steps:\n",
      "  - This cohort has essentially no observed death events, so overall survival cannot be separated by any clustering. If metastasis-free survival is of interest, we could redefine the event using TIME_TO_MET_MONTHS and an appropriate event indicator, then rerun the survival separation.\n",
      "  - With only 12 patients and very low event rates, robust survival-based separation is unlikely. Consider expanding the cohort, incorporating additional genomic features (e.g., copy number, structural variants), or focusing on alternative outcomes (recurrence or metastasis).\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "agent.register_workspace(\n",
    "    os.path.join(REPO_BASE_DIR, \"biomedical_data/cBioPortal/datasets/acbc_mskcc_2015\")\n",
    ")\n",
    "execution_results = agent.go(\"Make a clustering of the patients based on their genomic mutation data to maximize the separation of the prognostic survival outcomes.\")\n",
    "print(execution_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f40a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:25:23,278 - INFO - Downloaded biodsa to test_artifacts\n",
      "2025-12-23 14:25:23,286 - INFO - Downloaded biodsa_tools.tar.gz to test_artifacts\n",
      "2025-12-23 14:25:23,289 - INFO - Downloaded data_clinical_patient.csv to test_artifacts\n",
      "2025-12-23 14:25:23,293 - INFO - Downloaded data_clinical_sample.csv to test_artifacts\n",
      "2025-12-23 14:25:23,297 - INFO - Downloaded data_cna.csv to test_artifacts\n",
      "2025-12-23 14:25:23,301 - INFO - Downloaded data_gene_panel_matrix.csv to test_artifacts\n",
      "2025-12-23 14:25:23,305 - INFO - Downloaded data_mutations.csv to test_artifacts\n",
      "2025-12-23 14:25:23,308 - INFO - Downloaded data_sv.csv to test_artifacts\n",
      "2025-12-23 14:25:23,312 - INFO - Downloaded patient_cluster_assignments.csv to test_artifacts\n",
      "2025-12-23 14:25:23,316 - INFO - Downloaded selected_genes.csv to test_artifacts\n",
      "2025-12-23 14:25:23,320 - INFO - Downloaded survival_summary.csv to test_artifacts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biodsa', 'biodsa_tools.tar.gz', 'data_clinical_patient.csv', 'data_clinical_sample.csv', 'data_cna.csv', 'data_gene_panel_matrix.csv', 'data_mutations.csv', 'data_sv.csv', 'patient_cluster_assignments.csv', 'selected_genes.csv', 'survival_summary.csv']\n"
     ]
    }
   ],
   "source": [
    "# Download artifacts separately\n",
    "# Note: only when you have docker installed and running you can do this, otherwise it will raise an error\n",
    "print(execution_results.download_artifacts(output_dir=\"test_artifacts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde854da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:25:23,524 - INFO - Downloaded biodsa to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,532 - INFO - Downloaded biodsa_tools.tar.gz to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,535 - INFO - Downloaded data_clinical_patient.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,538 - INFO - Downloaded data_clinical_sample.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,542 - INFO - Downloaded data_cna.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,546 - INFO - Downloaded data_gene_panel_matrix.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,549 - INFO - Downloaded data_mutations.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,552 - INFO - Downloaded data_sv.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,555 - INFO - Downloaded patient_cluster_assignments.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,558 - INFO - Downloaded selected_genes.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n",
      "2025-12-23 14:25:23,561 - INFO - Downloaded survival_summary.csv to /var/folders/rb/nj5lt0x53pj4nt6j_b459_p80000gn/T/biodsa_artifacts_maad56nb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_artifacts/execution_report_20251223_142523.pdf\n"
     ]
    }
   ],
   "source": [
    "# Generate PDF report following the structured format:\n",
    "# 1. User query\n",
    "# 2. Agent exploration trajectories (messages only, no code)\n",
    "# 3. Final response with embedded artifacts\n",
    "# 4. Supplementary materials with code blocks and execution results\n",
    "print(execution_results.to_pdf(output_dir=\"test_artifacts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610fe072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 14:25:23,785 - INFO - Removed 6 artifacts of 6\n",
      "2025-12-23 14:25:23,928 - INFO - Successfully pruned unused volumes\n",
      "2025-12-23 14:25:23,929 - INFO - Sandbox stopped and resources cleaned up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6b630866fb75\n",
      "6b630866fb75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "agent.clear_workspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8080a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioDSA-dev--wbcjBPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
